[
  {
    "type": "message",
    "subtype": "channel_topic",
    "ts": "1588700419.000600",
    "user": "UC2A2ARPT",
    "text": "<@UC2A2ARPT> set the channel topic: Discussion of all types of FP, Category Theory, Denotational Design, and related work.",
    "topic": "Discussion of all types of FP, Category Theory, Denotational Design, and related work."
  },
  {
    "client_msg_id": "59e84285-f732-49c5-b9db-2f45cc8b8c53",
    "type": "message",
    "text": "When did functional programming begin?\n• 1952, \"The Use of Sub-Routines in Programmes\", D.J.Wheeler, <https://dl.acm.org/doi/pdf/10.1145/609784.609816> Mentions higher order functions like \"integrate\", which is (somehow) parameterized by the function to be integrated.\n• 1956, Fortran, John Backus. FORTRAN=FORmula TRANslation. Fortran supports nested arithmetic expressions. \"Programming on the right side of the assignment statement\" is an early glimpse of expression-oriented functional programming, as mentioned later in [Landin 1966].\n• 1960, LISP, John McCarthy, \"_Recursive functions of symbolic expressions and their computation by machine, part I_\". Lisp is the first programming language with conditional expressions, recursive functions, higher order functions (including \"map\" under the name \"maplist\"), plus garbage collection. This is the true birth of functional programming, even though LISP has a strong imperative core, and some key elements are missing: LISP is not lexically scoped; there are no closures, and no convenient syntax for curried function calls. <http://www-formal.stanford.edu/jmc/recursive.pdf>\n• 1962, APL, Kenneth Iverson, the book \"A Programming Language\" is published. Has an exceptionally powerful expression language, including the now standard \"map\" and \"reduce\" higher order functions (under different names). There is no APL interpreter yet. <http://www.softwarepreservation.org/projects/apl/Books/APROGRAMMING%20LANGUAGE>\n• 1964, P.J.Landin, \"The Mechanical Evaluation of Expressions\" describes a referentially transparent, lexically scoped, pure functional language with lexically scoped closures and curried functions. The syntax is a recognizable precursor to ML and Haskell. A virtual machine called the SECD machine is described for implementing functional languages (including lexical scoping and closures, which are missing from LISP). Earliest use of \"referential transparency\" to describe a programming language? <https://www.cs.cmu.edu/~crary/819-f09/Landin64.pdf>\n• 1966, P.J.Landin, \"The Next 700 Programming Languages\". This is the earliest published use of the term \"functional programming\" I can find, although it is by now reportedly in common use. This is the original _manifesto_ for functional (ie, non-imperative) programming. It describes the research language ISWIM, spawning the ML/Haskell lineage of functional languages. <http://www-formal.stanford.edu/jmc/recursive.pdf>",
    "user": "UJN1TAYEQ",
    "ts": "1588813244.019700",
    "team": "T5TCAFTA9",
    "user_team": "T5TCAFTA9",
    "source_team": "T5TCAFTA9",
    "user_profile": {
      "avatar_hash": "g4185a542241",
      "image_72": "https://secure.gravatar.com/avatar/34185a5422416f82b3e4a62964f2866b.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0008-72.png",
      "first_name": "",
      "real_name": "Doug Moen",
      "display_name": "Doug Moen",
      "team": "T5TCAFTA9",
      "name": "doug",
      "is_restricted": false,
      "is_ultra_restricted": false
    },
    "blocks": [
      {
        "type": "rich_text",
        "block_id": "acg9",
        "elements": [
          {
            "type": "rich_text_section",
            "elements": [
              {
                "type": "text",
                "text": "When did functional programming begin?\n"
              }
            ]
          },
          {
            "type": "rich_text_list",
            "elements": [
              {
                "type": "rich_text_section",
                "elements": [
                  {
                    "type": "text",
                    "text": "1952, \"The Use of Sub-Routines in Programmes\", D.J.Wheeler, "
                  },
                  {
                    "type": "link",
                    "url": "https://dl.acm.org/doi/pdf/10.1145/609784.609816"
                  },
                  {
                    "type": "text",
                    "text": " Mentions higher order functions like \"integrate\", which is (somehow) parameterized by the function to be integrated."
                  }
                ]
              },
              {
                "type": "rich_text_section",
                "elements": [
                  {
                    "type": "text",
                    "text": "1956, Fortran, John Backus. FORTRAN=FORmula TRANslation. Fortran supports nested arithmetic expressions. \"Programming on the right side of the assignment statement\" is an early glimpse of expression-oriented functional programming, as mentioned later in [Landin 1966]."
                  }
                ]
              },
              {
                "type": "rich_text_section",
                "elements": [
                  {
                    "type": "text",
                    "text": "1960, LISP, John McCarthy, \""
                  },
                  {
                    "type": "text",
                    "text": "Recursive functions of symbolic expressions and their computation by machine, part I",
                    "style": {
                      "italic": true
                    }
                  },
                  {
                    "type": "text",
                    "text": "\". Lisp is the first programming language with conditional expressions, recursive functions, higher order functions (including \"map\" under the name \"maplist\"), plus garbage collection. This is the true birth of functional programming, even though LISP has a strong imperative core, and some key elements are missing: LISP is not lexically scoped; there are no closures, and no convenient syntax for curried function calls. "
                  },
                  {
                    "type": "link",
                    "url": "http://www-formal.stanford.edu/jmc/recursive.pdf"
                  }
                ]
              },
              {
                "type": "rich_text_section",
                "elements": [
                  {
                    "type": "text",
                    "text": "1962, APL, Kenneth Iverson, the book \"A Programming Language\" is published. Has an exceptionally powerful expression language, including the now standard \"map\" and \"reduce\" higher order functions (under different names). There is no APL interpreter yet. "
                  },
                  {
                    "type": "link",
                    "url": "http://www.softwarepreservation.org/projects/apl/Books/APROGRAMMING%20LANGUAGE"
                  }
                ]
              },
              {
                "type": "rich_text_section",
                "elements": [
                  {
                    "type": "text",
                    "text": "1964, P.J.Landin, \"The Mechanical Evaluation of Expressions\" describes a referentially transparent, lexically scoped, pure functional language with lexically scoped closures and curried functions. The syntax is a recognizable precursor to ML and Haskell. A virtual machine called the SECD machine is described for implementing functional languages (including lexical scoping and closures, which are missing from LISP). Earliest use of \"referential transparency\" to describe a programming language? "
                  },
                  {
                    "type": "link",
                    "url": "https://www.cs.cmu.edu/~crary/819-f09/Landin64.pdf"
                  }
                ]
              },
              {
                "type": "rich_text_section",
                "elements": [
                  {
                    "type": "text",
                    "text": "1966, P.J.Landin, \"The Next 700 Programming Languages\". This is the earliest published use of the term \"functional programming\" I can find, although it is by now reportedly in common use. This is the original "
                  },
                  {
                    "type": "text",
                    "text": "manifesto",
                    "style": {
                      "italic": true
                    }
                  },
                  {
                    "type": "text",
                    "text": " for functional (ie, non-imperative) programming. It describes the research language ISWIM, spawning the ML/Haskell lineage of functional languages. "
                  },
                  {
                    "type": "link",
                    "url": "http://www-formal.stanford.edu/jmc/recursive.pdf"
                  }
                ]
              }
            ],
            "style": "bullet",
            "indent": 0
          }
        ]
      }
    ],
    "thread_ts": "1588813244.019700",
    "reply_count": 3,
    "reply_users_count": 3,
    "latest_reply": "1588856960.020700",
    "reply_users": [
      "UC2A2ARPT",
      "UA14TGLTC",
      "UJN1TAYEQ"
    ],
    "replies": [
      {
        "user": "UC2A2ARPT",
        "ts": "1588822484.019900"
      },
      {
        "user": "UA14TGLTC",
        "ts": "1588840584.020400"
      },
      {
        "user": "UJN1TAYEQ",
        "ts": "1588856960.020700"
      }
    ],
    "subscribed": false
  },
  {
    "client_msg_id": "219e926c-c186-4e62-9ed1-c8d023595244",
    "type": "message",
    "text": "Something I'm curious about, along these lines — what were the first computer languages that were in acknowledged as being related to / derived from lambda calculus?\n\nAnything prior to this? 1965, P.J. Landin, \"Correspondence between ALGOL 60 and Church's Lambda-notation\" <https://dl.acm.org/doi/10.1145/363744.363749>\n\nMy curiosity stems from.. wondering at what point people in the nascent PL world realized that there correspondences with the work being done to pin down math's foundations.",
    "user": "UC2A2ARPT",
    "ts": "1588822484.019900",
    "team": "T5TCAFTA9",
    "user_team": "T5TCAFTA9",
    "source_team": "T5TCAFTA9",
    "user_profile": {
      "avatar_hash": "gf94d2ed5e18",
      "image_72": "https://secure.gravatar.com/avatar/6f94d2ed5e188be9865a531021b0afcd.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0013-72.png",
      "first_name": "Ivan",
      "real_name": "Ivan Reese",
      "display_name": "",
      "team": "T5TCAFTA9",
      "name": "ivanreese",
      "is_restricted": false,
      "is_ultra_restricted": false
    },
    "edited": {
      "user": "UC2A2ARPT",
      "ts": "1588822707.000000"
    },
    "blocks": [
      {
        "type": "rich_text",
        "block_id": "o8V",
        "elements": [
          {
            "type": "rich_text_section",
            "elements": [
              {
                "type": "text",
                "text": "Something I'm curious about, along these lines — what were the first computer languages that were in acknowledged as being related to / derived from lambda calculus?\n\nAnything prior to this? 1965, P.J. Landin, \"Correspondence between ALGOL 60 and Church's Lambda-notation\" "
              },
              {
                "type": "link",
                "url": "https://dl.acm.org/doi/10.1145/363744.363749"
              },
              {
                "type": "text",
                "text": "\n\nMy curiosity stems from.. wondering at what point people in the nascent PL world realized that there correspondences with the work being done to pin down math's foundations."
              }
            ]
          }
        ]
      }
    ],
    "thread_ts": "1588813244.019700",
    "parent_user_id": "UJN1TAYEQ"
  },
  {
    "client_msg_id": "f0573478-13fd-491e-aaf2-539b5deb807f",
    "type": "message",
    "text": "For me FP hits its stride somewhere between non-strict evaluation and type classes.",
    "user": "UA14TGLTC",
    "ts": "1588840584.020400",
    "team": "T5TCAFTA9",
    "user_team": "T5TCAFTA9",
    "source_team": "T5TCAFTA9",
    "user_profile": {
      "avatar_hash": "gae6d55db9d1",
      "image_72": "https://secure.gravatar.com/avatar/3ae6d55db9d15b79bc683a8031fc2588.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0009-72.png",
      "first_name": "",
      "real_name": "William Taysom",
      "display_name": "wtaysom",
      "team": "T5TCAFTA9",
      "name": "wtaysom",
      "is_restricted": false,
      "is_ultra_restricted": false
    },
    "blocks": [
      {
        "type": "rich_text",
        "block_id": "yMyp",
        "elements": [
          {
            "type": "rich_text_section",
            "elements": [
              {
                "type": "text",
                "text": "For me FP hits its stride somewhere between non-strict evaluation and type classes."
              }
            ]
          }
        ]
      }
    ],
    "thread_ts": "1588813244.019700",
    "parent_user_id": "UJN1TAYEQ"
  },
  {
    "client_msg_id": "cb05f601-01f5-4918-b24a-f08db5c93a66",
    "type": "message",
    "text": "<@UC2A2ARPT> After Fortran exploded onto the scene in 1956, the record suggests that in the late 1950's, computer scientists on both sides of the Atlantic were working on programming languages inspired by lambda calculus.\n\nThe lambda calculus is a mathematical formalism: it is a model of the \"recursive functions\" aka the \"computable functions\". It is not a programming language. For one thing, there is no defined order of evaluation. Two popular choices are applicative order (most programming languages) and normal order (Algol 60 and Haskell). Normal order evaluation is more powerful, because it terminates and produces a result in cases where applicative order evaluation fails to terminate.\n\nOn the American side, McCarthy was a theoretician, and LISP was originally intended to be a mathematical model of the recursive functions, heavily inspired by lambda calculus, that was also executable on a computer. The ability to represent lambda expressions as data structures (\"S-expressions\" or symbolic expressions) was a key idea of Lisp. It led to Lisp FEXPRs and macros. But the identification of functions with LAMBDA expressions prevented early LISP from representing function values as closures or supporting lexical scoping. LISP has always used applicative order evaluation (except for FEXPRs and MACROs).\n\nAcross the Atlantic, Algol 60 was the successor to Algol 58 (aka IAL). In Algol 60, function parameters by default use \"call by name\" as the parameter passing mechanism (normal order evaluation). If you explicitly use the \"value\" keyword then you get \"call by value\", aka applicative order evaluation. It's just like using the ! operator in Haskell to force a function parameter to be strict. Allegedly, this strange choice of \"call by name\" as the default parameter passing mechanism was directly inspired by lambda calculus, where normal order evaluation is more powerful. I'd like to know who put normal order evaluation into the Algol 60 standard.\n\nHowever, Algol 60 is an imperative language, and the interaction between call-by-name and side effects made programs difficult to understand. The performance was also terrible, so call-by name gained a bad reputation and was dropped. It didn't reappear (AFAIK) until Haskell.",
    "user": "UJN1TAYEQ",
    "ts": "1588856960.020700",
    "team": "T5TCAFTA9",
    "user_team": "T5TCAFTA9",
    "source_team": "T5TCAFTA9",
    "user_profile": {
      "avatar_hash": "g4185a542241",
      "image_72": "https://secure.gravatar.com/avatar/34185a5422416f82b3e4a62964f2866b.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0008-72.png",
      "first_name": "",
      "real_name": "Doug Moen",
      "display_name": "Doug Moen",
      "team": "T5TCAFTA9",
      "name": "doug",
      "is_restricted": false,
      "is_ultra_restricted": false
    },
    "edited": {
      "user": "UJN1TAYEQ",
      "ts": "1588857670.000000"
    },
    "blocks": [
      {
        "type": "rich_text",
        "block_id": "+xT",
        "elements": [
          {
            "type": "rich_text_section",
            "elements": [
              {
                "type": "user",
                "user_id": "UC2A2ARPT"
              },
              {
                "type": "text",
                "text": " After Fortran exploded onto the scene in 1956, the record suggests that in the late 1950's, computer scientists on both sides of the Atlantic were working on programming languages inspired by lambda calculus.\n\nThe lambda calculus is a mathematical formalism: it is a model of the \"recursive functions\" aka the \"computable functions\". It is not a programming language. For one thing, there is no defined order of evaluation. Two popular choices are applicative order (most programming languages) and normal order (Algol 60 and Haskell). Normal order evaluation is more powerful, because it terminates and produces a result in cases where applicative order evaluation fails to terminate.\n\nOn the American side, McCarthy was a theoretician, and LISP was originally intended to be a mathematical model of the recursive functions, heavily inspired by lambda calculus, that was also executable on a computer. The ability to represent lambda expressions as data structures (\"S-expressions\" or symbolic expressions) was a key idea of Lisp. It led to Lisp FEXPRs and macros. But the identification of functions with LAMBDA expressions prevented early LISP from representing function values as closures or supporting lexical scoping. LISP has always used applicative order evaluation (except for FEXPRs and MACROs).\n\nAcross the Atlantic, Algol 60 was the successor to Algol 58 (aka IAL). In Algol 60, function parameters by default use \"call by name\" as the parameter passing mechanism (normal order evaluation). If you explicitly use the \"value\" keyword then you get \"call by value\", aka applicative order evaluation. It's just like using the ! operator in Haskell to force a function parameter to be strict. Allegedly, this strange choice of \"call by name\" as the default parameter passing mechanism was directly inspired by lambda calculus, where normal order evaluation is more powerful. I'd like to know who put normal order evaluation into the Algol 60 standard.\n\nHowever, Algol 60 is an imperative language, and the interaction between call-by-name and side effects made programs difficult to understand. The performance was also terrible, so call-by name gained a bad reputation and was dropped. It didn't reappear (AFAIK) until Haskell."
              }
            ]
          }
        ]
      }
    ],
    "thread_ts": "1588813244.019700",
    "parent_user_id": "UJN1TAYEQ"
  },
  {
    "client_msg_id": "a244770a-24b9-41e2-98cf-60d667b47f11",
    "type": "message",
    "text": "Anyone know of any \"toy\" optimizing compilers for a functional programming language that are small enough to study and fit in head? Looking for learning resources.\n\nTangentially, are there any nanopass compilers that are used outaide of education?",
    "user": "UT60XSVCN",
    "ts": "1589940714.024800",
    "team": "T5TCAFTA9",
    "user_team": "T5TCAFTA9",
    "source_team": "T5TCAFTA9",
    "user_profile": {
      "avatar_hash": "gbc3e6041047",
      "image_72": "https://secure.gravatar.com/avatar/bc3e6041047849518d1b042f0a29d5af.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0020-72.png",
      "first_name": "",
      "real_name": "S.M Mukarram Nainar",
      "display_name": "S.M Mukarram Nainar",
      "team": "T5TCAFTA9",
      "name": "nainars",
      "is_restricted": false,
      "is_ultra_restricted": false
    },
    "blocks": [
      {
        "type": "rich_text",
        "block_id": "cDEE",
        "elements": [
          {
            "type": "rich_text_section",
            "elements": [
              {
                "type": "text",
                "text": "Anyone know of any \"toy\" optimizing compilers for a functional programming language that are small enough to study and fit in head? Looking for learning resources.\n\nTangentially, are there any nanopass compilers that are used outaide of education?"
              }
            ]
          }
        ]
      }
    ],
    "thread_ts": "1589940714.024800",
    "reply_count": 12,
    "reply_users_count": 6,
    "latest_reply": "1592502105.031300",
    "reply_users": [
      "UA14TGLTC",
      "U78TZ437S",
      "UJN1TAYEQ",
      "UAVCC2X70",
      "UT60XSVCN",
      "U012QKESJF6"
    ],
    "replies": [
      {
        "user": "UA14TGLTC",
        "ts": "1589946647.024900"
      },
      {
        "user": "U78TZ437S",
        "ts": "1589966170.025100"
      },
      {
        "user": "UJN1TAYEQ",
        "ts": "1589977533.025300"
      },
      {
        "user": "UJN1TAYEQ",
        "ts": "1589977699.025500"
      },
      {
        "user": "UJN1TAYEQ",
        "ts": "1589977899.025800"
      },
      {
        "user": "UJN1TAYEQ",
        "ts": "1589978095.026000"
      },
      {
        "user": "UJN1TAYEQ",
        "ts": "1589978796.026300"
      },
      {
        "user": "UJN1TAYEQ",
        "ts": "1589979597.026600"
      },
      {
        "user": "UJN1TAYEQ",
        "ts": "1589979839.026900"
      },
      {
        "user": "UAVCC2X70",
        "ts": "1590000821.027100"
      },
      {
        "user": "UT60XSVCN",
        "ts": "1590084064.027300"
      },
      {
        "user": "U012QKESJF6",
        "ts": "1592502105.031300"
      }
    ],
    "subscribed": false
  },
  {
    "client_msg_id": "40f9f29c-23d1-4581-b57d-a844064ae676",
    "type": "message",
    "text": "Mentioning \"nanopass\" suggests that I wouldn't have much to add.  :wink:",
    "user": "UA14TGLTC",
    "ts": "1589946647.024900",
    "team": "T5TCAFTA9",
    "user_team": "T5TCAFTA9",
    "source_team": "T5TCAFTA9",
    "user_profile": {
      "avatar_hash": "gae6d55db9d1",
      "image_72": "https://secure.gravatar.com/avatar/3ae6d55db9d15b79bc683a8031fc2588.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0009-72.png",
      "first_name": "",
      "real_name": "William Taysom",
      "display_name": "wtaysom",
      "team": "T5TCAFTA9",
      "name": "wtaysom",
      "is_restricted": false,
      "is_ultra_restricted": false
    },
    "blocks": [
      {
        "type": "rich_text",
        "block_id": "ynh",
        "elements": [
          {
            "type": "rich_text_section",
            "elements": [
              {
                "type": "text",
                "text": "Mentioning \"nanopass\" suggests that I wouldn't have much to add.  "
              },
              {
                "type": "emoji",
                "name": "wink"
              }
            ]
          }
        ]
      }
    ],
    "thread_ts": "1589940714.024800",
    "parent_user_id": "UT60XSVCN"
  },
  {
    "client_msg_id": "e81c0622-3cb7-4138-b0a4-8decb9465509",
    "type": "message",
    "text": "For a small Hindley Milner (Algorithm W) type checker check out this introductory paper: <http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.65.7733&amp;rep=rep1&amp;type=pdf>\n\nand Lennart Augustusson has a few toy compilers for various small languages in his blog, e.g:\n<http://augustss.blogspot.com/2007/06/simple-compiler-in-my-last-post-i.html>",
    "user": "U78TZ437S",
    "ts": "1589966170.025100",
    "team": "T5TCAFTA9",
    "user_team": "T5TCAFTA9",
    "source_team": "T5TCAFTA9",
    "user_profile": {
      "avatar_hash": "ga3a20a76a69",
      "image_72": "https://secure.gravatar.com/avatar/da3a20a76a69532fa83e790e89cb4c6c.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0025-72.png",
      "first_name": "Eyal",
      "real_name": "Eyal Lotem",
      "display_name": "eyal",
      "team": "T5TCAFTA9",
      "name": "eyal.lotem",
      "is_restricted": false,
      "is_ultra_restricted": false
    },
    "blocks": [
      {
        "type": "rich_text",
        "block_id": "9Lc",
        "elements": [
          {
            "type": "rich_text_section",
            "elements": [
              {
                "type": "text",
                "text": "For a small Hindley Milner (Algorithm W) type checker check out this introductory paper: "
              },
              {
                "type": "link",
                "url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.65.7733&rep=rep1&type=pdf"
              },
              {
                "type": "text",
                "text": "\n\nand Lennart Augustusson has a few toy compilers for various small languages in his blog, e.g:\n"
              },
              {
                "type": "link",
                "url": "http://augustss.blogspot.com/2007/06/simple-compiler-in-my-last-post-i.html"
              }
            ]
          }
        ]
      }
    ],
    "thread_ts": "1589940714.024800",
    "parent_user_id": "UT60XSVCN"
  },
  {
    "client_msg_id": "5eca530e-98f2-4183-a8b6-1a4633f66977",
    "type": "message",
    "text": "\"A nanopass framework for commercial compiler development\" (for Chez Scheme) by Andy Keep. <http://andykeep.com/pubs/dissertation.pdf>",
    "user": "UJN1TAYEQ",
    "ts": "1589977533.025300",
    "team": "T5TCAFTA9",
    "user_team": "T5TCAFTA9",
    "source_team": "T5TCAFTA9",
    "user_profile": {
      "avatar_hash": "g4185a542241",
      "image_72": "https://secure.gravatar.com/avatar/34185a5422416f82b3e4a62964f2866b.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0008-72.png",
      "first_name": "",
      "real_name": "Doug Moen",
      "display_name": "Doug Moen",
      "team": "T5TCAFTA9",
      "name": "doug",
      "is_restricted": false,
      "is_ultra_restricted": false
    },
    "blocks": [
      {
        "type": "rich_text",
        "block_id": "VnA",
        "elements": [
          {
            "type": "rich_text_section",
            "elements": [
              {
                "type": "text",
                "text": "\"A nanopass framework for commercial compiler development\" (for Chez Scheme) by Andy Keep. "
              },
              {
                "type": "link",
                "url": "http://andykeep.com/pubs/dissertation.pdf"
              }
            ]
          }
        ]
      }
    ],
    "thread_ts": "1589940714.024800",
    "parent_user_id": "UT60XSVCN"
  },
  {
    "client_msg_id": "35de1cd2-de7c-4aa7-b001-9d2c08418cde",
    "type": "message",
    "text": "This project is open source and active. <http://nanopass.org/> and <https://github.com/nanopass>",
    "user": "UJN1TAYEQ",
    "ts": "1589977699.025500",
    "team": "T5TCAFTA9",
    "user_team": "T5TCAFTA9",
    "source_team": "T5TCAFTA9",
    "user_profile": {
      "avatar_hash": "g4185a542241",
      "image_72": "https://secure.gravatar.com/avatar/34185a5422416f82b3e4a62964f2866b.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0008-72.png",
      "first_name": "",
      "real_name": "Doug Moen",
      "display_name": "Doug Moen",
      "team": "T5TCAFTA9",
      "name": "doug",
      "is_restricted": false,
      "is_ultra_restricted": false
    },
    "attachments": [
      {
        "footer": "<https://github.com/nanopass|@nanopass>",
        "id": 1,
        "thumb_height": 205,
        "thumb_width": 205,
        "thumb_url": "https://avatars0.githubusercontent.com/u/13802051?v=4",
        "footer_icon": "https://github.githubassets.com/favicon.ico",
        "ts": 1439581535,
        "color": "24292f",
        "fields": [
          {
            "title": "URL",
            "value": "<http://nanopass.org>",
            "short": true
          },
          {
            "title": "Repositories",
            "value": "3",
            "short": true
          }
        ],
        "fallback": "[no preview available]",
        "bot_id": "B011KHY4N3Y",
        "app_unfurl_url": "https://github.com/nanopass",
        "is_app_unfurl": true
      }
    ],
    "blocks": [
      {
        "type": "rich_text",
        "block_id": "T+RW",
        "elements": [
          {
            "type": "rich_text_section",
            "elements": [
              {
                "type": "text",
                "text": "This project is open source and active. "
              },
              {
                "type": "link",
                "url": "http://nanopass.org/"
              },
              {
                "type": "text",
                "text": " and "
              },
              {
                "type": "link",
                "url": "https://github.com/nanopass"
              }
            ]
          }
        ]
      }
    ],
    "thread_ts": "1589940714.024800",
    "parent_user_id": "UT60XSVCN"
  },
  {
    "client_msg_id": "46e49398-7976-4085-93a6-33cc613a104a",
    "type": "message",
    "text": "This is something I want to learn more about myself. I'm not happy with the compiler I wrote for my own functional language.",
    "user": "UJN1TAYEQ",
    "ts": "1589977899.025800",
    "team": "T5TCAFTA9",
    "user_team": "T5TCAFTA9",
    "source_team": "T5TCAFTA9",
    "user_profile": {
      "avatar_hash": "g4185a542241",
      "image_72": "https://secure.gravatar.com/avatar/34185a5422416f82b3e4a62964f2866b.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0008-72.png",
      "first_name": "",
      "real_name": "Doug Moen",
      "display_name": "Doug Moen",
      "team": "T5TCAFTA9",
      "name": "doug",
      "is_restricted": false,
      "is_ultra_restricted": false
    },
    "blocks": [
      {
        "type": "rich_text",
        "block_id": "nx5M",
        "elements": [
          {
            "type": "rich_text_section",
            "elements": [
              {
                "type": "text",
                "text": "This is something I want to learn more about myself. I'm not happy with the compiler I wrote for my own functional language."
              }
            ]
          }
        ]
      }
    ],
    "thread_ts": "1589940714.024800",
    "parent_user_id": "UT60XSVCN"
  },
  {
    "client_msg_id": "b73bc745-a711-4d37-b957-ce7b98d82780",
    "type": "message",
    "text": "This blog post compares the nanopass approach to building a parser using parser combinators. &lt;<https://blog.sigplan.org/2019/07/09/my-first-fifteen-compilers/>&gt; I hope the analogy is correct, because building a compiler by composing simple generic functional building blocks sounds like just the approach I need.",
    "user": "UJN1TAYEQ",
    "ts": "1589978095.026000",
    "team": "T5TCAFTA9",
    "user_team": "T5TCAFTA9",
    "source_team": "T5TCAFTA9",
    "user_profile": {
      "avatar_hash": "g4185a542241",
      "image_72": "https://secure.gravatar.com/avatar/34185a5422416f82b3e4a62964f2866b.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0008-72.png",
      "first_name": "",
      "real_name": "Doug Moen",
      "display_name": "Doug Moen",
      "team": "T5TCAFTA9",
      "name": "doug",
      "is_restricted": false,
      "is_ultra_restricted": false
    },
    "attachments": [
      {
        "service_name": "SIGPLAN Blog",
        "title": "My First Fifteen Compilers",
        "title_link": "https://blog.sigplan.org/2019/07/09/my-first-fifteen-compilers/",
        "text": "We sometimes think of the number of passes in a compiler as a measure of the compiler’s complexity. But what if we could make compiler development more approachable by fully embracing the id…",
        "fallback": "SIGPLAN Blog: My First Fifteen Compilers",
        "image_url": "https://blog.sigplan.org/wp-content/uploads/2019/07/shutterstock_306120221.jpg",
        "image_width": 393,
        "image_height": 250,
        "ts": 1562677203,
        "from_url": "https://blog.sigplan.org/2019/07/09/my-first-fifteen-compilers/",
        "image_bytes": 5880099,
        "id": 1,
        "original_url": "https://blog.sigplan.org/2019/07/09/my-first-fifteen-compilers/"
      }
    ],
    "blocks": [
      {
        "type": "rich_text",
        "block_id": "sM4n",
        "elements": [
          {
            "type": "rich_text_section",
            "elements": [
              {
                "type": "text",
                "text": "This blog post compares the nanopass approach to building a parser using parser combinators. <"
              },
              {
                "type": "link",
                "url": "https://blog.sigplan.org/2019/07/09/my-first-fifteen-compilers/"
              },
              {
                "type": "text",
                "text": "> I hope the analogy is correct, because building a compiler by composing simple generic functional building blocks sounds like just the approach I need."
              }
            ]
          }
        ]
      }
    ],
    "thread_ts": "1589940714.024800",
    "parent_user_id": "UT60XSVCN",
    "reactions": [
      {
        "name": "heart",
        "users": [
          "U5STGTB3J"
        ],
        "count": 1
      }
    ]
  },
  {
    "client_msg_id": "9908d8ac-179e-48bc-bac9-a462b2f10493",
    "type": "message",
    "text": "Digging more into the idea of constructing a compiler by composing combinators. Here's a one page compiler written this way in Haskell that \"accepts a Turing-complete language and produces WebAssembly\". It's not an optimizing compiler, though. <https://crypto.stanford.edu/~blynn/lambda/sk.html>",
    "user": "UJN1TAYEQ",
    "ts": "1589978796.026300",
    "team": "T5TCAFTA9",
    "user_team": "T5TCAFTA9",
    "source_team": "T5TCAFTA9",
    "user_profile": {
      "avatar_hash": "g4185a542241",
      "image_72": "https://secure.gravatar.com/avatar/34185a5422416f82b3e4a62964f2866b.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0008-72.png",
      "first_name": "",
      "real_name": "Doug Moen",
      "display_name": "Doug Moen",
      "team": "T5TCAFTA9",
      "name": "doug",
      "is_restricted": false,
      "is_ultra_restricted": false
    },
    "blocks": [
      {
        "type": "rich_text",
        "block_id": "HE6tc",
        "elements": [
          {
            "type": "rich_text_section",
            "elements": [
              {
                "type": "text",
                "text": "Digging more into the idea of constructing a compiler by composing combinators. Here's a one page compiler written this way in Haskell that \"accepts a Turing-complete language and produces WebAssembly\". It's not an optimizing compiler, though. "
              },
              {
                "type": "link",
                "url": "https://crypto.stanford.edu/~blynn/lambda/sk.html"
              }
            ]
          }
        ]
      }
    ],
    "thread_ts": "1589940714.024800",
    "parent_user_id": "UT60XSVCN"
  },
  {
    "client_msg_id": "1cc80492-e327-4f84-812f-8b112e3dfe86",
    "type": "message",
    "text": "According to somebody on Hacker News &lt;<https://news.ycombinator.com/item?id=15153956>&gt;,\n&gt; Chez Scheme [0] is written using the nanopass framework, and it's regarded as one of the fastest Scheme compilers in existence [1]. Before it was rewritten to use the nanopass system, Chez's compiler was known for its performance in terms of lines of code compiled per second; the rewrite slowed it down a bit, but the quality and performance of generated machine code improved. Andy Keep and Kent Dybvig wrote a paper about the project [2]. I haven't browsed the Chez source, but it's a good way to answer your question.\n&gt; [0] <https://github.com/cisco/ChezScheme>\n&gt; [1] <http://ecraven.github.io/r7rs-benchmarks/benchmark.html>\n&gt; [2] <https://www.cs.indiana.edu/~dyb/pubs/commercial-nanopass.pdf>",
    "user": "UJN1TAYEQ",
    "ts": "1589979597.026600",
    "team": "T5TCAFTA9",
    "user_team": "T5TCAFTA9",
    "source_team": "T5TCAFTA9",
    "user_profile": {
      "avatar_hash": "g4185a542241",
      "image_72": "https://secure.gravatar.com/avatar/34185a5422416f82b3e4a62964f2866b.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0008-72.png",
      "first_name": "",
      "real_name": "Doug Moen",
      "display_name": "Doug Moen",
      "team": "T5TCAFTA9",
      "name": "doug",
      "is_restricted": false,
      "is_ultra_restricted": false
    },
    "attachments": [
      {
        "fallback": "cisco/ChezScheme",
        "text": "Chez Scheme ",
        "title": "cisco/ChezScheme",
        "footer": "<https://github.com/cisco/ChezScheme|cisco/ChezScheme>",
        "id": 1,
        "footer_icon": "https://github.githubassets.com/favicon.ico",
        "ts": 1460661025,
        "color": "24292f",
        "fields": [
          {
            "title": "Stars",
            "value": "5348",
            "short": true
          },
          {
            "title": "Language",
            "value": "Scheme",
            "short": true
          }
        ],
        "mrkdwn_in": [
          "text",
          "fields"
        ],
        "bot_id": "B011KHY4N3Y",
        "app_unfurl_url": "https://github.com/cisco/ChezScheme",
        "is_app_unfurl": true
      }
    ],
    "blocks": [
      {
        "type": "rich_text",
        "block_id": "Fv9Z",
        "elements": [
          {
            "type": "rich_text_section",
            "elements": [
              {
                "type": "text",
                "text": "According to somebody on Hacker News <"
              },
              {
                "type": "link",
                "url": "https://news.ycombinator.com/item?id=15153956"
              },
              {
                "type": "text",
                "text": ">,\n"
              }
            ]
          },
          {
            "type": "rich_text_quote",
            "elements": [
              {
                "type": "text",
                "text": "Chez Scheme [0] is written using the nanopass framework, and it's regarded as one of the fastest Scheme compilers in existence [1]. Before it was rewritten to use the nanopass system, Chez's compiler was known for its performance in terms of lines of code compiled per second; the rewrite slowed it down a bit, but the quality and performance of generated machine code improved. Andy Keep and Kent Dybvig wrote a paper about the project [2]. I haven't browsed the Chez source, but it's a good way to answer your question.\n[0] "
              },
              {
                "type": "link",
                "url": "https://github.com/cisco/ChezScheme"
              },
              {
                "type": "text",
                "text": "\n[1] "
              },
              {
                "type": "link",
                "url": "http://ecraven.github.io/r7rs-benchmarks/benchmark.html"
              },
              {
                "type": "text",
                "text": "\n[2] "
              },
              {
                "type": "link",
                "url": "https://www.cs.indiana.edu/~dyb/pubs/commercial-nanopass.pdf"
              }
            ]
          }
        ]
      }
    ],
    "thread_ts": "1589940714.024800",
    "parent_user_id": "UT60XSVCN"
  },
  {
    "client_msg_id": "6d7138b8-493c-436b-9902-09ed368e8381",
    "type": "message",
    "text": "Looking at the ChezScheme repository on github, it uses the nanopass framework on github (that I linked to earlier) as a dependency.",
    "user": "UJN1TAYEQ",
    "ts": "1589979839.026900",
    "team": "T5TCAFTA9",
    "user_team": "T5TCAFTA9",
    "source_team": "T5TCAFTA9",
    "user_profile": {
      "avatar_hash": "g4185a542241",
      "image_72": "https://secure.gravatar.com/avatar/34185a5422416f82b3e4a62964f2866b.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0008-72.png",
      "first_name": "",
      "real_name": "Doug Moen",
      "display_name": "Doug Moen",
      "team": "T5TCAFTA9",
      "name": "doug",
      "is_restricted": false,
      "is_ultra_restricted": false
    },
    "blocks": [
      {
        "type": "rich_text",
        "block_id": "vrJuU",
        "elements": [
          {
            "type": "rich_text_section",
            "elements": [
              {
                "type": "text",
                "text": "Looking at the ChezScheme repository on github, it uses the nanopass framework on github (that I linked to earlier) as a dependency."
              }
            ]
          }
        ]
      }
    ],
    "thread_ts": "1589940714.024800",
    "parent_user_id": "UT60XSVCN"
  },
  {
    "client_msg_id": "38c11b23-5a32-42d0-b54a-d6b7394fbc54",
    "type": "message",
    "text": "I really liked this programming language walkthrough for compiling a \"regular looking\" (looks imperative) language to continuation passing style (behaves like scheme + CPS):\n\n<http://lisperator.net/pltut/|http://lisperator.net/pltut/>",
    "user": "UAVCC2X70",
    "ts": "1590000821.027100",
    "team": "T5TCAFTA9",
    "user_team": "T5TCAFTA9",
    "source_team": "T5TCAFTA9",
    "user_profile": {
      "avatar_hash": "ec35ac5b251f",
      "image_72": "https://avatars.slack-edge.com/2018-05-22/369438371830_ec35ac5b251f37e26aca_72.jpg",
      "first_name": "",
      "real_name": "Dan Cook",
      "display_name": "",
      "team": "T5TCAFTA9",
      "name": "dcook0819",
      "is_restricted": false,
      "is_ultra_restricted": false
    },
    "blocks": [
      {
        "type": "rich_text",
        "block_id": "jBnm",
        "elements": [
          {
            "type": "rich_text_section",
            "elements": [
              {
                "type": "text",
                "text": "I really liked this programming language walkthrough for compiling a \"regular looking\" (looks imperative) language to continuation passing style (behaves like scheme + CPS):\n\n"
              },
              {
                "type": "link",
                "url": "http://lisperator.net/pltut/",
                "text": "http://lisperator.net/pltut/"
              }
            ]
          }
        ]
      }
    ],
    "thread_ts": "1589940714.024800",
    "parent_user_id": "UT60XSVCN"
  },
  {
    "client_msg_id": "a4fe373b-c7ed-4e7e-ac26-1802caac497c",
    "type": "message",
    "text": "Thanks everyone!\nI didn't know chez was rewritten in nanopass, that's neat.",
    "user": "UT60XSVCN",
    "ts": "1590084064.027300",
    "team": "T5TCAFTA9",
    "user_team": "T5TCAFTA9",
    "source_team": "T5TCAFTA9",
    "user_profile": {
      "avatar_hash": "gbc3e6041047",
      "image_72": "https://secure.gravatar.com/avatar/bc3e6041047849518d1b042f0a29d5af.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0020-72.png",
      "first_name": "",
      "real_name": "S.M Mukarram Nainar",
      "display_name": "S.M Mukarram Nainar",
      "team": "T5TCAFTA9",
      "name": "nainars",
      "is_restricted": false,
      "is_ultra_restricted": false
    },
    "blocks": [
      {
        "type": "rich_text",
        "block_id": "JvT",
        "elements": [
          {
            "type": "rich_text_section",
            "elements": [
              {
                "type": "text",
                "text": "Thanks everyone!\nI didn't know chez was rewritten in nanopass, that's neat."
              }
            ]
          }
        ]
      }
    ],
    "thread_ts": "1589940714.024800",
    "parent_user_id": "UT60XSVCN"
  },
  {
    "client_msg_id": "abf8df98-74cf-41c9-b0e9-fdb25a6de575",
    "type": "message",
    "text": "Anyone here implementing a functional language themselves?\n\nI'm interested in: how you do lazy evaluation and how you do any parallelisation (or is that concurrency?)",
    "user": "UE6EFEPTQ",
    "ts": "1591010264.028900",
    "team": "T5TCAFTA9",
    "user_team": "T5TCAFTA9",
    "source_team": "T5TCAFTA9",
    "user_profile": {
      "avatar_hash": "8073c43d5d8d",
      "image_72": "https://avatars.slack-edge.com/2018-12-18/508431502471_8073c43d5d8dd3d3b4b2_72.jpg",
      "first_name": "Duncan",
      "real_name": "Duncan Cragg",
      "display_name": "Duncan Cragg",
      "team": "T5TCAFTA9",
      "name": "fp",
      "is_restricted": false,
      "is_ultra_restricted": false
    },
    "blocks": [
      {
        "type": "rich_text",
        "block_id": "i2/",
        "elements": [
          {
            "type": "rich_text_section",
            "elements": [
              {
                "type": "text",
                "text": "Anyone here implementing a functional language themselves?\n\nI'm interested in: how you do lazy evaluation and how you do any parallelisation (or is that concurrency?)"
              }
            ]
          }
        ]
      }
    ],
    "thread_ts": "1591010264.028900",
    "reply_count": 9,
    "reply_users_count": 3,
    "latest_reply": "1591097607.030700",
    "reply_users": [
      "U78TZ437S",
      "UJN1TAYEQ",
      "UE6EFEPTQ"
    ],
    "replies": [
      {
        "user": "U78TZ437S",
        "ts": "1591011718.029000"
      },
      {
        "user": "UJN1TAYEQ",
        "ts": "1591033694.029300"
      },
      {
        "user": "UE6EFEPTQ",
        "ts": "1591090165.029500"
      },
      {
        "user": "UE6EFEPTQ",
        "ts": "1591090207.029700"
      },
      {
        "user": "U78TZ437S",
        "ts": "1591095684.029900"
      },
      {
        "user": "UJN1TAYEQ",
        "ts": "1591095731.030100"
      },
      {
        "user": "UE6EFEPTQ",
        "ts": "1591095799.030300"
      },
      {
        "user": "U78TZ437S",
        "ts": "1591095853.030500"
      },
      {
        "user": "UJN1TAYEQ",
        "ts": "1591097607.030700"
      }
    ],
    "subscribed": false
  },
  {
    "client_msg_id": "4ae9679c-4715-4067-b79a-71c233f687cf",
    "type": "message",
    "text": "We're implementing an eager language (Lamdu), with explicit laziness (actually call by name) via ordinary lam-of-unit.\n\nFor example, a lazy \"List\" type uses a lam-of-unit around the \"rest of list elements\".\n\nFor concurrency, we compile to nodejs, and use its concurrency model behind the scenes. But we hide the callbacks behind our \"IO\" monad (called Mut) so that the callbacks are just the ordinary monadic bind arguments",
    "user": "U78TZ437S",
    "ts": "1591011718.029000",
    "team": "T5TCAFTA9",
    "user_team": "T5TCAFTA9",
    "source_team": "T5TCAFTA9",
    "user_profile": {
      "avatar_hash": "ga3a20a76a69",
      "image_72": "https://secure.gravatar.com/avatar/da3a20a76a69532fa83e790e89cb4c6c.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0025-72.png",
      "first_name": "Eyal",
      "real_name": "Eyal Lotem",
      "display_name": "eyal",
      "team": "T5TCAFTA9",
      "name": "eyal.lotem",
      "is_restricted": false,
      "is_ultra_restricted": false
    },
    "blocks": [
      {
        "type": "rich_text",
        "block_id": "R+rT",
        "elements": [
          {
            "type": "rich_text_section",
            "elements": [
              {
                "type": "text",
                "text": "We're implementing an eager language (Lamdu), with explicit laziness (actually call by name) via ordinary lam-of-unit.\n\nFor example, a lazy \"List\" type uses a lam-of-unit around the \"rest of list elements\".\n\nFor concurrency, we compile to nodejs, and use its concurrency model behind the scenes. But we hide the callbacks behind our \"IO\" monad (called Mut) so that the callbacks are just the ordinary monadic bind arguments"
              }
            ]
          }
        ]
      }
    ],
    "thread_ts": "1591010264.028900",
    "parent_user_id": "UE6EFEPTQ",
    "reactions": [
      {
        "name": "+1",
        "users": [
          "U79HM6726"
        ],
        "count": 1
      }
    ]
  },
  {
    "client_msg_id": "ae699aba-2a36-4d89-8bca-8f75038cdcbe",
    "type": "message",
    "text": "I am implementing a dynamically typed, pure functional language (<http://github.com/curv3d/curv|github.com/curv3d/curv>). It's a work in progress.\n\nData parallelism and concurrency are quite different.\n\nMy language is data parallel. It compiles into data parallel GPU code (GLSL shader programs) or C++. Curv is an array language with array operations that can be executed in parallel, eg using hardware vector instructions. And since all functions are pure, there is no problem mapping a function over each element of an array, and executing all of the function calls in parallel on different cores (calls to pure functions can't interfere with each other via side effects).\n\nGeneral purpose concurrent programming isn't a goal, although I am planning to add support for creating user interfaces via some form of Functional Reactive Programming, and UI programming normally involves concurrency.\n\nHaskell style lazy evaluation is incompatible with my design goals, so it probably won't be added to Curv. As a result, I'm biased, and I don't consider Haskell-style lazy evaluation to be essential to pure functional programming. The real essence is denotative semantics, referential transparency, equational reasoning.\n\nA key problem is that Haskell's model of lazy evaluation is inherently single threaded. It can't be implemented efficiently on a GPU. It is incompatible with data parallelism. There are also problems relating to ease of use, debuggability, the developers ability to understand memory consumption and performance.\n\nFunction calls are strict in Curv. Aside from this being a requirement for GPU compilation, I also think that the downsides of lazy function calls outweigh the benefits, and I don't think that normal order evaluation should be the default.\n\nHaskell lazy lists are a special case of a more general idea of \"procedural data structures\". These are values that behave like data structures, but code is executed when you access data structure elements. Haskell lazy lists are inherently sequential and single threaded, and are impossible to implement on a GPU. I am more interested in exploring a general notion of procedural data that supports data parallelism and can be executed on a GPU.\n\n<@U78TZ437S> mentions that explicit laziness in Lamdu is actually \"call by name\". This is a form of laziness that is compatible with data parallelism. There is some \"call by name\" semantics buried in the Curv implementation, but I so far haven't found the need to document it and expose it in a language primitive. But that might happen once the language is fully designed and implemented.",
    "user": "UJN1TAYEQ",
    "ts": "1591033694.029300",
    "team": "T5TCAFTA9",
    "user_team": "T5TCAFTA9",
    "source_team": "T5TCAFTA9",
    "user_profile": {
      "avatar_hash": "g4185a542241",
      "image_72": "https://secure.gravatar.com/avatar/34185a5422416f82b3e4a62964f2866b.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0008-72.png",
      "first_name": "",
      "real_name": "Doug Moen",
      "display_name": "Doug Moen",
      "team": "T5TCAFTA9",
      "name": "doug",
      "is_restricted": false,
      "is_ultra_restricted": false
    },
    "blocks": [
      {
        "type": "rich_text",
        "block_id": "vCgLb",
        "elements": [
          {
            "type": "rich_text_section",
            "elements": [
              {
                "type": "text",
                "text": "I am implementing a dynamically typed, pure functional language ("
              },
              {
                "type": "link",
                "url": "http://github.com/curv3d/curv",
                "text": "github.com/curv3d/curv"
              },
              {
                "type": "text",
                "text": "). It's a work in progress.\n\nData parallelism and concurrency are quite different.\n\nMy language is data parallel. It compiles into data parallel GPU code (GLSL shader programs) or C++. Curv is an array language with array operations that can be executed in parallel, eg using hardware vector instructions. And since all functions are pure, there is no problem mapping a function over each element of an array, and executing all of the function calls in parallel on different cores (calls to pure functions can't interfere with each other via side effects).\n\nGeneral purpose concurrent programming isn't a goal, although I am planning to add support for creating user interfaces via some form of Functional Reactive Programming, and UI programming normally involves concurrency.\n\nHaskell style lazy evaluation is incompatible with my design goals, so it probably won't be added to Curv. As a result, I'm biased, and I don't consider Haskell-style lazy evaluation to be essential to pure functional programming. The real essence is denotative semantics, referential transparency, equational reasoning.\n\nA key problem is that Haskell's model of lazy evaluation is inherently single threaded. It can't be implemented efficiently on a GPU. It is incompatible with data parallelism. There are also problems relating to ease of use, debuggability, the developers ability to understand memory consumption and performance.\n\nFunction calls are strict in Curv. Aside from this being a requirement for GPU compilation, I also think that the downsides of lazy function calls outweigh the benefits, and I don't think that normal order evaluation should be the default.\n\nHaskell lazy lists are a special case of a more general idea of \"procedural data structures\". These are values that behave like data structures, but code is executed when you access data structure elements. Haskell lazy lists are inherently sequential and single threaded, and are impossible to implement on a GPU. I am more interested in exploring a general notion of procedural data that supports data parallelism and can be executed on a GPU.\n\n"
              },
              {
                "type": "user",
                "user_id": "U78TZ437S"
              },
              {
                "type": "text",
                "text": " mentions that explicit laziness in Lamdu is actually \"call by name\". This is a form of laziness that is compatible with data parallelism. There is some \"call by name\" semantics buried in the Curv implementation, but I so far haven't found the need to document it and expose it in a language primitive. But that might happen once the language is fully designed and implemented."
              }
            ]
          }
        ]
      }
    ],
    "thread_ts": "1591010264.028900",
    "parent_user_id": "UE6EFEPTQ"
  },
  {
    "client_msg_id": "528f36f5-e143-443a-b7ce-c64eb8359c60",
    "type": "message",
    "text": "Well, I'm still quite new to this in a way, but as I see it, both lazy evaluation and parallel term reduction are meant to increase efficiency (don't eval stuff you don't need, don't eval the same thing twice; eval in parallel if they're independent).\n\nSo putting the specifics of Haskell and Lamdu choices and implementation aside, I was wondering if it's possible to have a model of graph/DAG reduction that is both lazy and parallelisable?\n\nFirst thought is that you need a results lookup that can be shared amongst threads. A thread would have to \"claim\" a result slot even before evaluating it, to prevent two threads seeing no result and eval'ing simultaneously.\n\nAnyway, just shower thoughts, I'm not building this yet..",
    "user": "UE6EFEPTQ",
    "ts": "1591090165.029500",
    "team": "T5TCAFTA9",
    "user_team": "T5TCAFTA9",
    "source_team": "T5TCAFTA9",
    "user_profile": {
      "avatar_hash": "8073c43d5d8d",
      "image_72": "https://avatars.slack-edge.com/2018-12-18/508431502471_8073c43d5d8dd3d3b4b2_72.jpg",
      "first_name": "Duncan",
      "real_name": "Duncan Cragg",
      "display_name": "Duncan Cragg",
      "team": "T5TCAFTA9",
      "name": "fp",
      "is_restricted": false,
      "is_ultra_restricted": false
    },
    "blocks": [
      {
        "type": "rich_text",
        "block_id": "+UF",
        "elements": [
          {
            "type": "rich_text_section",
            "elements": [
              {
                "type": "text",
                "text": "Well, I'm still quite new to this in a way, but as I see it, both lazy evaluation and parallel term reduction are meant to increase efficiency (don't eval stuff you don't need, don't eval the same thing twice; eval in parallel if they're independent).\n\nSo putting the specifics of Haskell and Lamdu choices and implementation aside, I was wondering if it's possible to have a model of graph/DAG reduction that is both lazy and parallelisable?\n\nFirst thought is that you need a results lookup that can be shared amongst threads. A thread would have to \"claim\" a result slot even before evaluating it, to prevent two threads seeing no result and eval'ing simultaneously.\n\nAnyway, just shower thoughts, I'm not building this yet.."
              }
            ]
          }
        ]
      }
    ],
    "thread_ts": "1591010264.028900",
    "parent_user_id": "UE6EFEPTQ"
  },
  {
    "client_msg_id": "6f3c11d2-abbc-4b79-9ce7-eef935d95429",
    "type": "message",
    "text": "I guess the lookup is like a bucket of thunks",
    "user": "UE6EFEPTQ",
    "ts": "1591090207.029700",
    "team": "T5TCAFTA9",
    "user_team": "T5TCAFTA9",
    "source_team": "T5TCAFTA9",
    "user_profile": {
      "avatar_hash": "8073c43d5d8d",
      "image_72": "https://avatars.slack-edge.com/2018-12-18/508431502471_8073c43d5d8dd3d3b4b2_72.jpg",
      "first_name": "Duncan",
      "real_name": "Duncan Cragg",
      "display_name": "Duncan Cragg",
      "team": "T5TCAFTA9",
      "name": "fp",
      "is_restricted": false,
      "is_ultra_restricted": false
    },
    "blocks": [
      {
        "type": "rich_text",
        "block_id": "oF+",
        "elements": [
          {
            "type": "rich_text_section",
            "elements": [
              {
                "type": "text",
                "text": "I guess the lookup is like a bucket of thunks"
              }
            ]
          }
        ]
      }
    ],
    "thread_ts": "1591010264.028900",
    "parent_user_id": "UE6EFEPTQ"
  },
  {
    "client_msg_id": "da5241c2-f094-4210-acd8-4c58cff8c3f1",
    "type": "message",
    "text": "I think the Haskell attempts at tackling auto-parallelism resulted in failures, and they found the difficult part to be granularity of work.\n\nCross-core talk is expensive, cache sharing and invalidation is expensive, locks/barriers are expensive -- all of this is overhead that better be paid for work that is large enough to justify it.\n\nInferring which work is large enough to be worth sending to a different processor is not an easy problem",
    "user": "U78TZ437S",
    "ts": "1591095684.029900",
    "team": "T5TCAFTA9",
    "user_team": "T5TCAFTA9",
    "source_team": "T5TCAFTA9",
    "user_profile": {
      "avatar_hash": "ga3a20a76a69",
      "image_72": "https://secure.gravatar.com/avatar/da3a20a76a69532fa83e790e89cb4c6c.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0025-72.png",
      "first_name": "Eyal",
      "real_name": "Eyal Lotem",
      "display_name": "eyal",
      "team": "T5TCAFTA9",
      "name": "eyal.lotem",
      "is_restricted": false,
      "is_ultra_restricted": false
    },
    "blocks": [
      {
        "type": "rich_text",
        "block_id": "YLiRx",
        "elements": [
          {
            "type": "rich_text_section",
            "elements": [
              {
                "type": "text",
                "text": "I think the Haskell attempts at tackling auto-parallelism resulted in failures, and they found the difficult part to be granularity of work.\n\nCross-core talk is expensive, cache sharing and invalidation is expensive, locks/barriers are expensive -- all of this is overhead that better be paid for work that is large enough to justify it.\n\nInferring which work is large enough to be worth sending to a different processor is not an easy problem"
              }
            ]
          }
        ]
      }
    ],
    "thread_ts": "1591010264.028900",
    "parent_user_id": "UE6EFEPTQ"
  },
  {
    "client_msg_id": "859ab4fc-308b-40f3-bde8-c1adc5b51e67",
    "type": "message",
    "text": "<@UE6EFEPTQ> Lazy evaluation does not increase efficiency. Haskell people often say this, but it is pure marketing. Haskell's lazy evaluation decreases efficiency, but this was an intentional tradeoff, because the goal (of the original language designers) was to increase expressive power. There's so much to say about this. I'll drop one factoid. Idris is an offshoot of Haskell that is designed to have even more expressive power than Haskell, due to its dependent type system. But Idris uses strict evaluation by default. The Idris 2 compiler was rewritten in Idris (formerly written in Haskell), and now runs 15 times faster. Lazy evaluation in the old Haskell version was a major contributor to slowness. <https://www.type-driven.org.uk/edwinb/why-is-idris-2-so-much-faster-than-idris-1.html>\n\nI'm not an expert on graph reduction engines, but I have to ask: how high a priority is efficiency for you, vs ease of programming, and vs expressive power?",
    "user": "UJN1TAYEQ",
    "ts": "1591095731.030100",
    "team": "T5TCAFTA9",
    "user_team": "T5TCAFTA9",
    "source_team": "T5TCAFTA9",
    "user_profile": {
      "avatar_hash": "g4185a542241",
      "image_72": "https://secure.gravatar.com/avatar/34185a5422416f82b3e4a62964f2866b.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0008-72.png",
      "first_name": "",
      "real_name": "Doug Moen",
      "display_name": "Doug Moen",
      "team": "T5TCAFTA9",
      "name": "doug",
      "is_restricted": false,
      "is_ultra_restricted": false
    },
    "blocks": [
      {
        "type": "rich_text",
        "block_id": "CUbFz",
        "elements": [
          {
            "type": "rich_text_section",
            "elements": [
              {
                "type": "user",
                "user_id": "UE6EFEPTQ"
              },
              {
                "type": "text",
                "text": " Lazy evaluation does not increase efficiency. Haskell people often say this, but it is pure marketing. Haskell's lazy evaluation decreases efficiency, but this was an intentional tradeoff, because the goal (of the original language designers) was to increase expressive power. There's so much to say about this. I'll drop one factoid. Idris is an offshoot of Haskell that is designed to have even more expressive power than Haskell, due to its dependent type system. But Idris uses strict evaluation by default. The Idris 2 compiler was rewritten in Idris (formerly written in Haskell), and now runs 15 times faster. Lazy evaluation in the old Haskell version was a major contributor to slowness. "
              },
              {
                "type": "link",
                "url": "https://www.type-driven.org.uk/edwinb/why-is-idris-2-so-much-faster-than-idris-1.html"
              },
              {
                "type": "text",
                "text": "\n\nI'm not an expert on graph reduction engines, but I have to ask: how high a priority is efficiency for you, vs ease of programming, and vs expressive power?"
              }
            ]
          }
        ]
      }
    ],
    "thread_ts": "1591010264.028900",
    "parent_user_id": "UE6EFEPTQ"
  },
  {
    "client_msg_id": "469ea067-8dda-4361-9f6f-9027686b1127",
    "type": "message",
    "text": "Well the goal is to be transparent to the programmer, just optimisation",
    "user": "UE6EFEPTQ",
    "ts": "1591095799.030300",
    "team": "T5TCAFTA9",
    "user_team": "T5TCAFTA9",
    "source_team": "T5TCAFTA9",
    "user_profile": {
      "avatar_hash": "8073c43d5d8d",
      "image_72": "https://avatars.slack-edge.com/2018-12-18/508431502471_8073c43d5d8dd3d3b4b2_72.jpg",
      "first_name": "Duncan",
      "real_name": "Duncan Cragg",
      "display_name": "Duncan Cragg",
      "team": "T5TCAFTA9",
      "name": "fp",
      "is_restricted": false,
      "is_ultra_restricted": false
    },
    "blocks": [
      {
        "type": "rich_text",
        "block_id": "SIX2",
        "elements": [
          {
            "type": "rich_text_section",
            "elements": [
              {
                "type": "text",
                "text": "Well the goal is to be transparent to the programmer, just optimisation"
              }
            ]
          }
        ]
      }
    ],
    "thread_ts": "1591010264.028900",
    "parent_user_id": "UE6EFEPTQ"
  },
  {
    "client_msg_id": "9c5d6e98-6a31-4514-b070-951fb6545a4a",
    "type": "message",
    "text": "Lazy evaluation adds overhead (inefficiency), but often the efficient solution is lazy.\n\nEager languages allow you to be lazy explicitly for the cases that's needed (e.g: for efficiency), and that's why <@UJN1TAYEQ> is right -- pervasive / implicit laziness isn't efficient.  But it can make code written with less effort perform better (if laziness is needed, you get it for no extra work)",
    "user": "U78TZ437S",
    "ts": "1591095853.030500",
    "team": "T5TCAFTA9",
    "user_team": "T5TCAFTA9",
    "source_team": "T5TCAFTA9",
    "user_profile": {
      "avatar_hash": "ga3a20a76a69",
      "image_72": "https://secure.gravatar.com/avatar/da3a20a76a69532fa83e790e89cb4c6c.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0025-72.png",
      "first_name": "Eyal",
      "real_name": "Eyal Lotem",
      "display_name": "eyal",
      "team": "T5TCAFTA9",
      "name": "eyal.lotem",
      "is_restricted": false,
      "is_ultra_restricted": false
    },
    "blocks": [
      {
        "type": "rich_text",
        "block_id": "bYm",
        "elements": [
          {
            "type": "rich_text_section",
            "elements": [
              {
                "type": "text",
                "text": "Lazy evaluation adds overhead (inefficiency), but often the efficient solution is lazy.\n\nEager languages allow you to be lazy explicitly for the cases that's needed (e.g: for efficiency), and that's why "
              },
              {
                "type": "user",
                "user_id": "UJN1TAYEQ"
              },
              {
                "type": "text",
                "text": " is right -- pervasive / implicit laziness isn't efficient.  But it can make code written with less effort perform better (if laziness is needed, you get it for no extra work)"
              }
            ]
          }
        ]
      }
    ],
    "thread_ts": "1591010264.028900",
    "parent_user_id": "UE6EFEPTQ"
  },
  {
    "client_msg_id": "2af19a79-a3db-4129-9802-b5a05c34ef0c",
    "type": "message",
    "text": "Data parallelism in Curv is explicit, because I don't know how to do otherwise and meet my performance goals. But the intention is that most Curv users will not need to deal with performance annotations: these will be hidden away behind high level interfaces, inside library code written by experts. So the language design goal is to allow non-experts to use optimized library abstractions without having to be explicitly aware of the optimization (you shouldn't need to change the way you write high level code to get a performance benefit). High level code is meant to be purely declarative.",
    "user": "UJN1TAYEQ",
    "ts": "1591097607.030700",
    "team": "T5TCAFTA9",
    "user_team": "T5TCAFTA9",
    "source_team": "T5TCAFTA9",
    "user_profile": {
      "avatar_hash": "g4185a542241",
      "image_72": "https://secure.gravatar.com/avatar/34185a5422416f82b3e4a62964f2866b.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0008-72.png",
      "first_name": "",
      "real_name": "Doug Moen",
      "display_name": "Doug Moen",
      "team": "T5TCAFTA9",
      "name": "doug",
      "is_restricted": false,
      "is_ultra_restricted": false
    },
    "blocks": [
      {
        "type": "rich_text",
        "block_id": "stA",
        "elements": [
          {
            "type": "rich_text_section",
            "elements": [
              {
                "type": "text",
                "text": "Data parallelism in Curv is explicit, because I don't know how to do otherwise and meet my performance goals. But the intention is that most Curv users will not need to deal with performance annotations: these will be hidden away behind high level interfaces, inside library code written by experts. So the language design goal is to allow non-experts to use optimized library abstractions without having to be explicitly aware of the optimization (you shouldn't need to change the way you write high level code to get a performance benefit). High level code is meant to be purely declarative."
              }
            ]
          }
        ]
      }
    ],
    "thread_ts": "1591010264.028900",
    "parent_user_id": "UE6EFEPTQ"
  },
  {
    "client_msg_id": "bb6d93e3-e081-4ec4-adba-fc1273abeab3",
    "type": "message",
    "text": "I think Lisp in Small Pieces or Appel's Compiling with Continuations might have some good basic optimisation techniques but I would also love to know a good resource for this!",
    "user": "U012QKESJF6",
    "ts": "1592502105.031300",
    "team": "T5TCAFTA9",
    "user_team": "T5TCAFTA9",
    "source_team": "T5TCAFTA9",
    "user_profile": {
      "avatar_hash": "g0d11c25a0fa",
      "image_72": "https://secure.gravatar.com/avatar/0d11c25a0fa037a73c22c65361fb142a.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0014-72.png",
      "first_name": "",
      "real_name": "Boarders",
      "display_name": "Boarders",
      "team": "T5TCAFTA9",
      "name": "callan.mcgill",
      "is_restricted": false,
      "is_ultra_restricted": false
    },
    "blocks": [
      {
        "type": "rich_text",
        "block_id": "01Q=I",
        "elements": [
          {
            "type": "rich_text_section",
            "elements": [
              {
                "type": "text",
                "text": "I think Lisp in Small Pieces or Appel's Compiling with Continuations might have some good basic optimisation techniques but I would also love to know a good resource for this!"
              }
            ]
          }
        ]
      }
    ],
    "thread_ts": "1589940714.024800",
    "parent_user_id": "UT60XSVCN"
  }
]